<!doctype html><html lang="en"><head><meta charset="UTF-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="ScholarNotion provides data-driven science topics, charts, insights, and transparent methodology." /><title>Transformer | ScholarNotion</title><link rel="stylesheet" href="/assets/css/styles.css" /></head><body><header class="site-header"><div class="container nav"><a class="brand" href="/">ScholarNotion</a><nav class="menu"><a href="/pages/topics.html">Topics</a><a href="/pages/charts.html">Charts</a><a href="/pages/insights.html">Insights</a><a href="/pages/explorer.html">Explorer</a><a href="/pages/methodology.html">Methodology</a></nav><a class="search-chip" href="/pages/charts.html">Search data</a></div></header><main class="container"><section class="page-title"><h1>Transformer</h1><p>Topic: Machine Learning Â· Last Updated: 2026-02-15</p></section><section class="section"><article class="card"><span class="pill">TL;DR</span><p>A transformer uses self-attention to model global dependencies in a sequence without recurrent state transitions.</p></article></section><section class="two-col section"><article class="card"><h3>Intuition</h3><p>Each token dynamically weighs all other relevant tokens to build contextual representation in one forward pass.</p></article><article class="card"><h3>Formal</h3><p><code>Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V</code></p></article></section><section class="two-col section"><article class="card"><h3>Example</h3><p>In translation, target token generation can reference multiple source positions rather than only local hidden state memory.</p></article><article class="card"><h3>Pitfalls</h3><ul class="list"><li>Attention weights are not direct causal explanations.</li><li>Compute and memory scale quadratically with sequence length.</li></ul></article></section></main></body></html>
