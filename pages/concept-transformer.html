<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Transformer | ScienceNotion</title>
    <link rel="stylesheet" href="/assets/css/styles.css" />
  </head>
  <body>
    <header class="site-header">
      <div class="container nav">
        <a class="brand" href="/">ScienceNotion</a>
        <nav class="menu">
          <a href="/pages/explore.html">Explore</a>
          <a href="/pages/news.html">News</a>
          <a href="/pages/wiki.html">Wiki</a>
          <a href="/pages/paths.html">Paths</a>
          <a href="/pages/topics.html">Topics</a>
          <a href="/pages/about.html">About</a>
        </nav>
      </div>
    </header>
    <main class="container">
      <section class="page-title">
        <h1>Transformer</h1>
        <p>Topic: Machine Learning · Type: Editorial · Last Updated: 2026-02-15</p>
      </section>
      <section class="section">
        <article class="card">
          <span class="pill">TL;DR</span>
          <p>Transformer 通过自注意力机制在序列内建立全局依赖，替代了传统 RNN 的逐步传递结构。</p>
        </article>
      </section>
      <section class="two-col section">
        <article class="card">
          <h3>Intuition</h3>
          <p>每个 token 会动态关注其他 token，从而在一个前向过程中获得上下文信息。</p>
        </article>
        <article class="card">
          <h3>Formal</h3>
          <p><code>Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V</code></p>
        </article>
      </section>
      <section class="two-col section">
        <article class="card">
          <h3>Example</h3>
          <p>在机器翻译中，目标词生成可关注源句中多个相关位置，而非只依赖局部隐藏态。</p>
        </article>
        <article class="card">
          <h3>Pitfalls</h3>
          <ul class="list">
            <li>把注意力权重误当作因果解释。</li>
            <li>忽略长序列时计算开销为 O(n^2)。</li>
          </ul>
        </article>
      </section>
      <section class="two-col section">
        <article class="card">
          <h3>Prerequisites</h3>
          <ul class="list">
            <li>向量表示与矩阵乘法</li>
            <li>概率分布与 softmax</li>
            <li>序列建模任务</li>
          </ul>
        </article>
        <article class="card">
          <h3>Further Reading</h3>
          <ul class="list">
            <li>Attention Is All You Need (2017)</li>
            <li>Scaling Laws for Neural Language Models (2020)</li>
          </ul>
        </article>
      </section>
      <section class="section">
        <article class="card">
          <h3>Changelog</h3>
          <ul class="list">
            <li>2026-02-15: 增加误区与前置概念。</li>
            <li>2026-02-12: 首版发布。</li>
          </ul>
        </article>
      </section>
    </main>
  </body>
</html>
